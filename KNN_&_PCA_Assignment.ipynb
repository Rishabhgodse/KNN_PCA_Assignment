{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIDpUmHNcVGB"
      },
      "outputs": [],
      "source": [
        "                                              #      KNN & PCA Assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1: What is K-Nearest Neighbors (KNN) and how does it work?"
      ],
      "metadata": {
        "id": "XMxda_4hcg8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a simple, instance-based machine learning algorithm used for classification and regression tasks. It works by finding the 'K' closest data points (neighbors) to the given input data and making predictions based on these neighbors. For classification, it assigns the class most common among the neighbors, and for regression, it calculates the average of the neighbors' values."
      ],
      "metadata": {
        "id": "hsCnC8chcmC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: What is the difference between KNN Classification and KNN Regression?"
      ],
      "metadata": {
        "id": "s-i96Znmcg46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Classification: Predicts a class label based on the majority vote among the 'K' nearest neighbors.\n",
        "\n",
        "KNN Regression: Predicts a continuous value by averaging the values of the 'K' nearest neighbors."
      ],
      "metadata": {
        "id": "IGuJ_dKFcspm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: What is the role of the distance metric in KNN?"
      ],
      "metadata": {
        "id": "zVq0jnKLcg2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distance metric determines how the neighbors are measured. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance. The choice of distance metric impacts how the algorithm finds the nearest neighbors."
      ],
      "metadata": {
        "id": "fpXR6P9ec0eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: What is the Curse of Dimensionality in KNN?"
      ],
      "metadata": {
        "id": "OANUr26Dcgzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Curse of Dimensionality refers to the problem where the performance of KNN decreases as the number of features (dimensions) increases. In high-dimensional spaces, data points become more spread out, making it difficult to find close neighbors."
      ],
      "metadata": {
        "id": "sTQdw2K7c6xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: How can we choose the best value of K in KNN?"
      ],
      "metadata": {
        "id": "wqigEGNpcgw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the best value of K is often done by experimentation, using techniques such as cross-validation. A small K might lead to overfitting, while a large K could underfit the data. Typically, an odd number is chosen to avoid ties in classification."
      ],
      "metadata": {
        "id": "8BwjyxzSdAQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: What are KD Tree and Ball Tree in KNN?"
      ],
      "metadata": {
        "id": "m0RDL3GscguK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KD Tree and Ball Tree are data structures used to speed up the search for nearest neighbors:\n",
        "\n",
        "KD Tree: A binary tree that partitions the data along axis-aligned hyperplanes.\n",
        "\n",
        "Ball Tree: A hierarchical structure that groups data points into hyperspheres. These structures allow for faster querying, especially in large datasets."
      ],
      "metadata": {
        "id": "xR1T_a9udGK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: When should you use KD Tree vs. Ball Tree?"
      ],
      "metadata": {
        "id": "BX_qjibdcgrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KD Tree: Works best with low-dimensional data (fewer than 20 dimensions).\n",
        "\n",
        "Ball Tree: Performs better with higher-dimensional data and non-axis-aligned data."
      ],
      "metadata": {
        "id": "TMqN4cSpdLy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8: What are the disadvantages of KNN?"
      ],
      "metadata": {
        "id": "0bpmiJ08cgo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High computational cost during prediction as the model needs to calculate the distance to all points.\n",
        "\n",
        "Sensitive to the choice of K.\n",
        "\n",
        "Poor performance in high-dimensional spaces due to the Curse of Dimensionality.\n",
        "\n",
        "Sensitive to noise and irrelevant features."
      ],
      "metadata": {
        "id": "peYKKVT1dRow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: How does feature scaling affect KNN?"
      ],
      "metadata": {
        "id": "QOwzVc30cgmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN is sensitive to the scale of features since it uses distance metrics to calculate nearest neighbors. Feature scaling (e.g., normalization or standardization) ensures that all features contribute equally to the distance metric, improving the performance of the algorithm."
      ],
      "metadata": {
        "id": "FdP4JAnodXrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: What is PCA (Principal Component Analysis)?"
      ],
      "metadata": {
        "id": "Seo674SScgjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by finding the directions (principal components) that maximize the variance in the data."
      ],
      "metadata": {
        "id": "7MFcnfXddtvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11: How does PCA work?"
      ],
      "metadata": {
        "id": "O3Gukeybcggz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA works by:\n",
        "\n",
        "Standardizing the data.\n",
        "\n",
        "Computing the covariance matrix of the data.\n",
        "\n",
        "Finding the eigenvalues and eigenvectors of the covariance matrix.\n",
        "\n",
        "Projecting the data onto the top 'k' eigenvectors (principal components) corresponding to the largest eigenvalues."
      ],
      "metadata": {
        "id": "Y38THv8Odx53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12: What is the geometric intuition behind PCA?"
      ],
      "metadata": {
        "id": "V4aWu0uscgeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometrically, PCA finds the directions (principal components) that capture the most variance in the data. It can be thought of as finding the axes of an ellipsoid that best represent the data's spread."
      ],
      "metadata": {
        "id": "U-xp3CLcd3Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13: What are Eigenvalues and Eigenvectors in PCA?"
      ],
      "metadata": {
        "id": "JYOIstqBcgbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvectors: Represent the directions (principal components) of the new feature space.\n",
        "\n",
        "Eigenvalues: Represent the magnitude of the variance captured by each principal component."
      ],
      "metadata": {
        "id": "6X3d-rlGd9Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14: What is the difference between Feature Selection and Feature Extraction?"
      ],
      "metadata": {
        "id": "KzUDrbmZcgZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection: Involves selecting a subset of the original features based on some criteria.\n",
        "\n",
        "Feature Extraction: Involves creating new features by transforming the original features (as in PCA) to capture the essential information."
      ],
      "metadata": {
        "id": "UYQmZ0fteDSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15: How do you decide the number of components to keep in PCA?"
      ],
      "metadata": {
        "id": "SV16vSrmcgWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of components to keep is often decided by retaining enough components to explain a certain percentage of the variance (e.g., 90-95%). This can be visualized using a scree plot."
      ],
      "metadata": {
        "id": "899vJyKseJRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16: Can PCA be used for classification?"
      ],
      "metadata": {
        "id": "avn9X7aacgTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA itself is not a classification algorithm, but it can be used as a preprocessing step for classification by reducing the dimensionality of the data, which helps in improving the performance of classification algorithms."
      ],
      "metadata": {
        "id": "5v4C6rE4eOjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17: What are the limitations of PCA?"
      ],
      "metadata": {
        "id": "LEAdnyo-cgRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA assumes that the data is linearly separable.\n",
        "\n",
        "It may lose important information, especially in cases where the variance does not capture the essential structure of the data.\n",
        "\n",
        "It may not perform well on highly noisy data or data where features have different variances."
      ],
      "metadata": {
        "id": "eWYpqP2veTz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18: How do KNN and PCA complement each other?"
      ],
      "metadata": {
        "id": "lQoYuUHTcgOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA can be used to reduce the dimensionality of the data before applying KNN. This can help mitigate the Curse of Dimensionality and improve the performance of KNN by reducing noise and irrelevant features."
      ],
      "metadata": {
        "id": "T3YDyyDQeZVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19: How does KNN handle missing values in a dataset?"
      ],
      "metadata": {
        "id": "sh_6oc4xcgLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN can handle missing values by imputing them. One common approach is to replace the missing values with the average or mode of the K-nearest neighbors' values for that feature."
      ],
      "metadata": {
        "id": "VO5PZEoqeevu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q20: What are the key differences between PCA and Linear Discriminant Analysis (LDA)?"
      ],
      "metadata": {
        "id": "gACY8JJ5cgJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA: Focuses on maximizing variance in the data, unsupervised.\n",
        "\n",
        "LDA: Focuses on maximizing class separability, supervised. LDA is better suited for classification problems where class labels are available, whereas PCA is used for general dimensionality reduction."
      ],
      "metadata": {
        "id": "0jKYe7o7ekea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                  # Practical"
      ],
      "metadata": {
        "id": "wJAbKZfScgGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21: Train a KNN Classifier on the Iris dataset and print model accuracy"
      ],
      "metadata": {
        "id": "nctkVgp9et7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN Classifier Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "kzaNz_KPfA-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22: Train a KNN Regressor on a synthetic dataset and evaluate using MSE"
      ],
      "metadata": {
        "id": "cURinEPWet4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=3)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Q2 - KNN Regressor MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "eZ4pyuDUfIYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23: Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy"
      ],
      "metadata": {
        "id": "s0-4zuX3et1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn_clf_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_clf_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "\n",
        "knn_clf_euclidean.fit(X_train, y_train)\n",
        "knn_clf_manhattan.fit(X_train, y_train)\n",
        "\n",
        "acc_euclidean = accuracy_score(y_test, knn_clf_euclidean.predict(X_test))\n",
        "acc_manhattan = accuracy_score(y_test, knn_clf_manhattan.predict(X_test))\n",
        "\n",
        "print(f\"Q3 - Euclidean Accuracy: {acc_euclidean * 100:.2f}%\")\n",
        "print(f\"Q3 - Manhattan Accuracy: {acc_manhattan * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "4eU3e9-hfTy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24: Train a KNN Classifier with different values of K and visualize decision boundaries"
      ],
      "metadata": {
        "id": "KeL9te6Ketz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k_values = [1, 5, 10]\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train[:, :2], y_train)\n",
        "    \n",
        "    # Plotting decision boundary\n",
        "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "    \n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', s=20)\n",
        "    plt.title(f'K = {k}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NG0eaY4yfcmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25: Apply Feature Scaling before training a KNN model and compare results with unscaled data"
      ],
      "metadata": {
        "id": "3ZR_gQAFetxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Q5 - Unscaled Accuracy: {acc_unscaled * 100:.2f}%\")\n",
        "print(f\"Q5 - Scaled Accuracy: {acc_scaled * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "kEyJktCUfmCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26: Train a PCA model on synthetic data and print the explained variance ratio for each component"
      ],
      "metadata": {
        "id": "hbNL2G-xetu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_synthetic, _ = make_regression(n_samples=100, n_features=5)\n",
        "pca = PCA(n_components=5)\n",
        "pca.fit(X_synthetic)\n",
        "\n",
        "print(f\"Q6 - Explained Variance Ratio: {pca.explained_variance_ratio_}\")"
      ],
      "metadata": {
        "id": "k2ljSFkifwAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: Apply PCA before training a KNN Classifier and compare accuracy with and without PCA"
      ],
      "metadata": {
        "id": "fYMg3K3_etsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "acc_no_pca = accuracy_score(y_test, knn_clf.predict(X_test))\n",
        "\n",
        "print(f\"Q7 - Accuracy with PCA: {acc_pca * 100:.2f}%\")\n",
        "print(f\"Q7 - Accuracy without PCA: {acc_no_pca * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "lYhUNGzkf8QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28: Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV"
      ],
      "metadata": {
        "id": "tpKgEPBietox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "param_grid = {'n_neighbors': [3, 5, 7, 10], 'metric': ['euclidean', 'manhattan']}\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Q8 - Best Params: {grid_search.best_params_}\")\n",
        "print(f\"Q8 - Best Score: {grid_search.best_score_}\")"
      ],
      "metadata": {
        "id": "71mRuzd7gIuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29: Train a KNN Classifier and check the number of misclassified samples"
      ],
      "metadata": {
        "id": "udODmUYgetmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y_pred_knn = knn_clf.predict(X_test)\n",
        "misclassified_samples = np.sum(y_test != y_pred_knn)\n",
        "\n",
        "print(f\"Q9 - Number of Misclassified Samples: {misclassified_samples}\")"
      ],
      "metadata": {
        "id": "x3X1dfBOgRne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30: Train a PCA model and visualize the cumulative explained variance"
      ],
      "metadata": {
        "id": "j_B_EsFUi58c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca = PCA().fit(X_synthetic)\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(cumulative_variance)\n",
        "plt.title('Q10 - Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3XQ31Tmi8xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31: Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy"
      ],
      "metadata": {
        "id": "5hMvCijGjAnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "knn_distance.fit(X_train, y_train)\n",
        "\n",
        "acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n",
        "acc_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n",
        "\n",
        "print(f\"Q1 - Uniform Accuracy: {acc_uniform * 100:.2f}%\")\n",
        "print(f\"Q1 - Distance Accuracy: {acc_distance * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "BT53wozWjgR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q32: Train a KNN Regressor and analyze the effect of different K values on performance"
      ],
      "metadata": {
        "id": "kCBTTCrpjp9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "k_values = [1, 3, 5, 7, 10]\n",
        "for k in k_values:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train, y_train)\n",
        "    y_pred = knn_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Q2 - K={k}, MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "NymSyrn_jv4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q33: Implement KNN Imputation for handling missing values in a dataset"
      ],
      "metadata": {
        "id": "U3FjPgsJj6l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating synthetic dataset with missing values\n",
        "\n",
        "X_missing, _ = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
        "X_missing[np.random.randint(0, 100, 20), np.random.randint(0, 2, 20)] = np.nan\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "X_imputed = imputer.fit_transform(X_missing)\n",
        "print(f\"Q3 - First 5 rows after Imputation:\\n{X_imputed[:5]}\")"
      ],
      "metadata": {
        "id": "O1jywoiBj_kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q34: Train a PCA model and visualize the data projection onto the first two principal components"
      ],
      "metadata": {
        "id": "aZ89ZThJkEeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_synthetic, _ = make_classification(n_samples=100, n_features=5)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_synthetic)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.title(\"Q4 - Data Projection on First Two PCA Components\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bXCjdv4Hk7KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q35: Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance"
      ],
      "metadata": {
        "id": "8rTmVGj1k-_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "\n",
        "acc_kd_tree = accuracy_score(y_test, knn_kd_tree.predict(X_test))\n",
        "acc_ball_tree = accuracy_score(y_test, knn_ball_tree.predict(X_test))\n",
        "\n",
        "print(f\"Q5 - KD Tree Accuracy: {acc_kd_tree * 100:.2f}%\")\n",
        "print(f\"Q5 - Ball Tree Accuracy: {acc_ball_tree * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "iBqIt8polHOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q36: Train a PCA model on a high-dimensional dataset and visualize the Scree plot"
      ],
      "metadata": {
        "id": "j1jQab9olKuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_high_dim, _ = make_classification(n_samples=200, n_features=20)\n",
        "pca_high_dim = PCA()\n",
        "pca_high_dim.fit(X_high_dim)\n",
        "\n",
        "plt.plot(np.cumsum(pca_high_dim.explained_variance_ratio_))\n",
        "plt.title(\"Q6 - Scree Plot of High-Dimensional Dataset\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8RPMzMjOlPQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q37: Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score"
      ],
      "metadata": {
        "id": "SB-dSx7tlUO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f\"Q7 - Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "y_yOi26HnC0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q38: Train a PCA model and analyze the effect of different numbers of components on accuracy"
      ],
      "metadata": {
        "id": "stzzT6MsnGVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca_components = [1, 2, 3, 4]\n",
        "for n in pca_components:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_pca.fit(X_train_pca, y_train)\n",
        "    \n",
        "    acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "    print(f\"Q8 - Components={n}, Accuracy: {acc_pca * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "JkkVWf_gnRXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q39: Train a KNN Classifier with different leaf_size values and compare accuracy"
      ],
      "metadata": {
        "id": "EGT7229nnVRP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "leaf_sizes = [10, 20, 30, 40]\n",
        "for leaf in leaf_sizes:\n",
        "    knn_leaf = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf)\n",
        "    knn_leaf.fit(X_train, y_train)\n",
        "    acc_leaf = accuracy_score(y_test, knn_leaf.predict(X_test))\n",
        "    print(f\"Q9 - Leaf Size={leaf}, Accuracy: {acc_leaf * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "ZPliPZLyna7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q40: Train a PCA model and visualize how data points are transformed before and after PCA"
      ],
      "metadata": {
        "id": "pGqdnPa3nhC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_synthetic_2D, _ = make_classification(n_samples=100, n_features=2)\n",
        "pca = PCA(n_components=1)\n",
        "X_pca_transformed = pca.fit_transform(X_synthetic_2D)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_synthetic_2D[:, 0], X_synthetic_2D[:, 1])\n",
        "plt.title(\"Q10 - Before PCA\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca_transformed, np.zeros_like(X_pca_transformed))\n",
        "plt.title(\"Q10 - After PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TFqBT3HNopoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q41: Train a KNN Classifier on the Wine dataset and print classification report"
      ],
      "metadata": {
        "id": "npddqMWSosD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_wine = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_wine.fit(X_train, y_train)\n",
        "y_pred_wine = knn_wine.predict(X_test)\n",
        "\n",
        "print(f\"Q11 - Classification Report:\\n{classification_report(y_test, y_pred_wine)}\")"
      ],
      "metadata": {
        "id": "72uBoUGeow6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q42: Train a KNN Regressor and analyze the effect of different distance metrics on prediction error"
      ],
      "metadata": {
        "id": "o3u2FS4Co0UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
        "for metric in metrics:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn_reg.fit(X_train, y_train)\n",
        "    y_pred = knn_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Q12 - Metric={metric}, MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "HJAwIi6fo5h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q43: Train a KNN Classifier and evaluate using ROC-AUC score"
      ],
      "metadata": {
        "id": "m5e8_wxqo8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_prob = knn_clf.predict_proba(X_test)[:, 1]  # For binary ROC-AUC\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "print(f\"Q13 - ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "GmFBMVmvo_8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q44: Train a PCA model and visualize the variance captured by each principal component"
      ],
      "metadata": {
        "id": "sMkA9Mm3pCUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
        "plt.title(\"Q14 - Variance Captured by Each Principal Component\")\n",
        "plt.xlabel(\"Principal Components\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLuXv2_npHlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q45: Train a KNN Classifier and perform feature selection before training"
      ],
      "metadata": {
        "id": "hojevraspJ6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=3)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "knn_fs = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_fs.fit(X_train_selected, y_train)\n",
        "\n",
        "acc_fs = accuracy_score(y_test, knn_fs.predict(X_test_selected))\n",
        "print(f\"Q15 - Accuracy after Feature Selection: {acc_fs * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "GodjCix2pNmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q46: Train a PCA model and visualize the data reconstruction error after reducing dimensions"
      ],
      "metadata": {
        "id": "fZ3L_CCXpQsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "reconstruction_error = np.mean((X_train - X_reconstructed) ** 2)\n",
        "print(f\"Q16 - Reconstruction Error: {reconstruction_error:.4f}\")"
      ],
      "metadata": {
        "id": "u3Ukozm-pX8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q47: Train a KNN Classifier and visualize the decision boundary"
      ],
      "metadata": {
        "id": "HaAlu5r8pbdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X_2D, y_2D = make_classification(n_samples=100, n_features=2, n_classes=3, n_informative=2, n_redundant=0)\n",
        "knn_clf_2D = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf_2D.fit(X_2D, y_2D)\n",
        "\n",
        "x_min, x_max = X_2D[:, 0].min() - 1, X_2D[:, 0].max() + 1\n",
        "y_min, y_max = X_2D[:, 1].min() - 1, X_2D[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "Z = knn_clf_2D.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap(('red', 'green', 'blue')))\n",
        "plt.scatter(X_2D[:, 0], X_2D[:, 1], c=y_2D, edgecolors='k', cmap=ListedColormap(('red', 'green', 'blue')))\n",
        "plt.title(\"Q17 - KNN Decision Boundary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tK3LJdONph2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18: Train a PCA model and analyze the effect of different numbers of components on data variance"
      ],
      "metadata": {
        "id": "FbF2cOpdplbW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca_variance = [1, 2, 3, 4]\n",
        "for n in pca_variance:\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X_train)\n",
        "    print(f\"Q18 - Components={n}, Variance Ratio: {np.sum(pca.explained_variance_ratio_):.4f}\")"
      ],
      "metadata": {
        "id": "T1y8LX5apr1E"
      }
    }
  ]
}